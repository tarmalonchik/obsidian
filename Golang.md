+ [Slices](#slices)
+ [Maps](#maps)
+ [Escape analysis](#escape%20analysis)
+ [Wait groups](#wait%20groups)
+ [Err groups](#err%20groups)
+ [Горутины](#горутины)
+ [GOMAXPROCS как работает](#gomaxprocs)
+ [GMP model](#gmp%20model)
+ [Go runtime](#go%20runtime)
+ [Channels](#channels)
+ [Обработка паник](#обработка%20паник)
+ [GC](#gc)
+ [Sync map](#sync%20map)
+ [Profiling](#profiling)
+ [Sync pool](#sync%20pool)
+ [ctx.WithTimeout() vs ctx.WithDeadline()](#ctx.WithTimeout()%20vs%20ctx.WithDeadline())
+ [Interfaces](#interfaces)
+ [errors.is vs errors.as](#errors.is%20vs%20errors.as)
+ [задачи](#задачи)

Задачи с подвохом

+ Смержить 3 канала в один
+ range по слайсу с указателем, известный баг
+ баг с захватом переменной, когда захватывается рандомный i


## Slices
+ При добавлении элементов в slice, сначала он растет в 2 раза, потом он начинает расти в 1.25 раз. Вроде как порог установлен на значение 256
+ Slice - это структура, состоящая из capacity, size и ссылки на родительский массив. 
+ Slicing не копирует данные slice. Просто создается новый slice, который указывает на оригинальный массив. Поэтому при изменении данных нового slice, данные меняются и в оригинальном slice. 
+ Амортизированная сложность добавления нового элемента в slice составляет O(1). При худшем сценарии, сложность составляет O(N). 
+ При append(), если размер нового slice становится больше, чем capacity, то происходит копирование slice в новую ячейку памяти. Размер новой ячейки памяти рассчитывается исходя из размеров оригинального slice. 

## Maps
+ Maps, это хеш таблица. Благодаря использовании хеш таблицы, доступ к элементам осуществляется за константное время. 
+ Для борьбы с коллизиями, внутри map, есть бакеты.
+ Итерация по map, не сохраняет порядок. Рандомизация для итерации была добавлена специально, чтобы разработчики не полагались на порядок данных в map. 
+ При добавлении данных в хеш таблицу, используется рандомизация выбора хеш функции. Это было сделано специально, чтобы исключить атаку (Hashmap collision attack). Это атака, при которой подбираются значения, которые будут вызывать множество коллизий, и хеш таблица будет расти слишком сильно. 
+ Понятие Load Factor - это отношение количества хранимых пар «ключ-значение» к размеру таблицы. Если они достигают пограничного значения, то хэш-мапу расширяют, а индексы пар пересчитывают и перераспределяют. Когда значение Load Factor достигает значения 80% (6.5 элементов при размере бакета 8), тогда начинается эвакуация данных. 
+ Если размер одного из бакетов начинает превышать 8, то к нему линкуется дополнительный бакет. 
+ Средняя сложность получения/вставки... O(1). Худшая сложность O(N). 
+ В качестве ключа, можно использовать любой тип, для которого определена операция сравнения (== && != тоже операции сравнения) 
+ При росте map, элементы переезжают в новую map постепенно, а не сразу. 


````
// Структура map в golang
type hmap struct { 
	count       int  
	B           uint 8  
	noverflow   uint 16  
	hash0       uint 32  
	buckets     unsafe.Pointer  
	oldbuckets  unsafe.Pointer
	nevacuate  uintptr
}
````
Где: 
+ count - live cells == size of map.  Must be first (used by len() builtin)
+ B - log_2 of # of buckets (can hold up to loadFactor * 2^B items)
+ noverflow - approximate number of overflow buckets; see incrnoverflow for details
+ **hash0** — hash seed
+ **buckets** — array of 2^B Buckets. may be nil if count == 0.
+ **oldbuckets** — previous bucket array of half the size, non-nil only when growing
+ nevacuate - progress counter for evacuation (buckets less than this have been evacuated)


## Escape analysis

Escape analysis - это определение того, где будет находится определенная переменная, в стеке или в heap. 

Stack vs Heap:

| Действие    | Стек                                                                               | Куча                                                                                                                   |
| ----------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| Аллокация   | O(1) - происходит просто сдвиг указателя                                           | O(logN) - поиск свободного места                                                                                       |
| Очищение    | LIFO (т.к адресное пространство непрерывное, то стек очищается за O(1))            | Очищением Heap занимается GC, и поэтому это снижает производительность. GC снижает производительность всей программы.  |
| Кеширование | Т.к данные локальные (т.е находятся в одном месте), то кешировать эти данные проще | Данные могут быть разбросаны, поэтому кеширование менее эффективно                                                     |
+ Где будет находиться переменная, определяется на этапе компиляции. 
+ Местонахождение переменной не важно для правильности работы программы, но важно для производительности. 
+ При Sharing Down, переменная, как правило, остается на стеке. 
+ При Sharing Up, переменная, как правило, попадает в heap.
+ Компилятор производит escape analysis, для определения того, куда пойдет переменна. 
+ Чтобы узнать, куда пойдет та или иная переменная, надо сделать команду `go build -gcflags "-m -l" main.go` (`-m=2` для получения более развернутой информации )

Когда переменная идет в heap?
1. Когда переменная (в теории), может быть вызвана, после завершения функции, которая породила данную переменную. 
2. Когда компилятор понимает, что значение слишком большое, для того, чтобы остаться на стеке. 
3. Когда компилятор не знает, какого размера будет переменная, на этапе компиляции. 

Типичные случая, когда переменная идет в heap. 
1. Переменные, на которые ссылаются указатели. 
2. Переменные, которые хранятся в Interface
3. Переменные, которым присваивается функция. 
4. Такие типы как Map, Channel, Slice, Strings

## Wait groups

Wait groups позволяют опеределить группу горутин, которые должны выполниться вместе, как одна группа. 
```
type WaitGroup struct {  
    noCopy noCopy  
  
    state atomic.Uint64
    sema  uint32  
}
```

```
func main() {
    var wg sync.WaitGroup

    for i := 1; i <= 5; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            worker(i)
        }()
    }
    wg.Wait()
}
```

+ Если значение WG становится отрицательным, golang вызывает panic. 
+ структура noCopy используется для линтеров и тд, для того, чтобы можно было определить, что данная переменная копируется, чего делать нельзя
+ WG хранит переменную sema. Данная переменна используется для функциями runtime. 
+ `runtime_Semacquire(&wg.sema)` данная функция блокирует горутину, до тех пор пока не будет вызвана функция `runtime_Semrelease(&wg.sema)`
+ использует atomic, для инкремента, декремента счетчика

## Err groups
```
func Run(runFunctions []func()error ) error {  
    ctx := context.Background()  
    eg, ctx := errgroup.WithContext(ctx)  
    eg.SetLimit(x)
    for i := range runFunctions {  
       eg.Go(runFunctions[i])  
    }  
    return eg.Wait()  
}
```
+ `errgroup.WithContext(ctx)` - внутри себя создает cancel func from ctx. 
+ `eg.Go` - вызывает функцию в новой горутине. Также данная функция может заблокироваться, в случае, если установлен лимит, и количество текущих горутин достигло лимита.  Если функция возвращает ошибку, то контекст отменяется. 
+ `eg.SetLimit(x)` - выставить лимит, для параллельно запущенных горутин. Лимит нельзя изменять, если есть запущенные горутины.
+ `eg.Wait()` - данный метод блокирует выполнение, ожидание до выполнения всех горутин. После выполнения, если произошла ошибки, то вернется первая ошибка из списка. 
## Горутины

Горутина — функция, которая может работать параллельно с другими функциями. Для создания горутины используется ключевое слово `go`, за которым следует вызов функции. Горутины очень легкие(примерно 4.5кб на горутину против нескольких мегобайт на поток POSIX).

Как правило, минимальный размер стека горутины составляет около 2кб (в разных версиях Go, минимальный размер стека менялся)

## GOMAXPROCS
Планировщик Go использует параметр с именем **GOMAXPROCS** для определения, сколько потоков операционной системы могут одновременно активно выполнять код Go. Его значение по умолчанию равно количеству процессоров компьютера, так что на машине с 8 процессорами планировщик будет планировать код Go для выполнения на 8 потоках одновременно (**GOMAXPROCS** равно значению п в т:п-планировании). Спящие или заблокированные в процессе коммуникации go-подпрограммы потоков для себя не требуют. Go-подпрограммы, заблокированные в операции ввода-вывода или в других системных вызовах, или при вызове функций, не являющихся функциями Go, нуждаются в потоке операционной системы, но **GOMAXPROCS** их не учитывает.

`runtime.GOMAXPROCS()`функция возвращает предыдущее значение для этого параметра. Вызов данной функции, вызывает stop the world. 

## GMP model
![](https://ghkdqhrbals.github.io/portfolios/assets/p/6/goroutine.png)
+ G - горутина
+ M - это поток операционной системы
+ P - это сущность планировщика golang, которая управляет горутинами
+ Когда у M кончаются G, тогда M пытается украсть половину G у другого M. 
+ Если у других M не получилось украсть G, тогда M идет в глобальную очередь, для того чтобы достать G оттуда. 
+ При обращении в глобальную очередь вызывается Lock, для безопасного доступа к глобальной очереди. 
+ Каждый 1/61 раз, M пытается достать G из глобальной очереди, вместо локальной. Это нужно для того, чтобы больше G распределялись равномерно между M. Число 61 выбрано, для того, чтобы снизить вероятность обращения разных M к глобальной очереди в одно и то же время. 
+ Когда M крадет работу у другого M, он все равно выполняет его G используя старый P. При этом между G и P не теряется связь. 
+ Когда происходит блокирующий вызов к сети, часто используется механизм network poller (epoll in linux). Это механизм ОС linux, который берет на себя часть взаимодействия с сетью. G передается network poller, и после этого он возвращается в локальную очередь??? Возможно G не возвращается в глобальную очередь, а ожидает в очереди Network Poller, до тех пор, пока его не заберут. 
+ Network poller - это компонент рантайма GO. Данный компонент нужен для исполнения IO опреаций, например при работе с сетью. Network poller под капотом использует функции ОС для работы с сетью. 
+ Когда происходит блокирующий системный вызов, то поток полностью блокируется и с этим нельзя ничего поделать. При блокировке потока, планировщик открепляет связку  M-G1 от P. После этого он создает/использует-старый  поток M2, который продолжает исполнять локальную очередь P. После завершения блокирующего вызова, G2 возвращается к M2 и добавляется в локальную очередь. M1 остается в стороне, для последующего использования. 
+ В golang до версии 1.14 использовалась кооперативная многозадачность. Это означало, что планировщик не мог заставить горутину отдать поток, и только сама горутина решала, когда это сделать. С версии 1.14 горутина больше не может работать дольше 10мс. Даже если горутина не успела завершить свою работу, планировщик все равно ее останавливает. 
+ При добавлении горутин в очередь, последний worker может выполнится первым. Это связано с тем, что высока вероятность того, что последний worker имеет общий контекс с P. Поэтому была сделана такая оптимизация. 
+ Если горутин не осталось, то поток засыпает, потом его может разбудить sys mon. 

Sysmon
+ Обнаруживает M зависшие в syscall  > 10ms
	+ если M завис в syscall, sysmon запускает новый M, чтобы P не простаивал
	+ когда syscall завершается, то G возвращается в очередь, а M либо продолжает работать, либо паркуется. 
+ Обслуживает network poller. То есть он проверяет неблокирующие IO (`net`, `epoll`, `kqueue`, `IOCP`), чекает сетевые события, и будет G, которые ждут данных. 
+ Пробуждает P, если он долго простаивает (если P ничего не делал в течение 10ms, то sysmon дает ему работу из глобальной очереди)
+ Запускает форсированный GC

## GO runtime
+ runtime.Gosched() -  этим вызовом, мы можем заставить планировщик переключить контекст. 
+ runtime.GOMAXPROCS() - с помощью этой функции, можно выставить максимальное количество потоков, которые будет использовать программа

## Sync map

Sync map появился в версии 1.9

У стандартного решения `map+sync.RWMutex` есть проблема, которая называется `cache contention`.  Если посмотреть на [код sync.RWMutex](https://golang.org/src/sync/rwmutex.go#L94), то можно увидеть, что при блокировке на чтение, каждая goroutine должна обновить поле `readerCount` — простой счётчик. Когда каждое ядро процессора обновляет счётчик, оно сбрасывает кеш для этого адреса в памяти для всех остальных ядер и объявляет, что владеет актуальным значением для этого адреса. Следующее ядро, прежде чем обновить счётчик, должно сначала вычитать это значение из кеша другого ядра. Это немного, но когда много ядер одновременно пытаются обновить счётчик, каждое из них становится в очередь и ждёт эту инвалидацию и вычитывание из кеша. Операция, которая должна укладываться в константное время внезапно становится O(N) по количеству ядер. Эта проблема называется `cache contention`.

`sync.Map` решает совершенно конкретную проблему cache contention в стандартной библиотеке для таких случаев, когда ключи в map стабильны (не обновляются часто) и происходит намного больше чтений, чем записей.

```
type Map struct {    
	mu Mutex    
	read atomic.Value // readOnly    
	dirty map[interface{}]*entry    
	misses int,
}
```

+ `sync.Map` нельзя копировать
+ `store` `sync.Map`. Сначала попытка обновить значение в `read`. Если не успех, то блокировка в `mu` и обновление `dirty`. 
+ `load` `sync.Map`. Сначала попытка найти значение в `read`. Если не нашлось, то блокировка `mu` и поиск в `dirty`. Также увеличивается счетчик `missies`. 
+ когда счетчик `missies` превысил длину `dirty`, тогда происходит следующее:
	+ захват `mu`
	+ copy `dirty` into `read`
	+ clean `dirty`
	+ `missies = 0`

## errors.is vs errors.as

+ errors.Is() shows, if error have the same value
+ errors.As() checks if errors have the same type
Для того чтобы данные функции работали, необходимо упаковать одну ошибку другую с помощью тега `%v`. Иначе упаковка не будет работать. 

## ctx.WithTimeout() vs ctx.WithDeadline()

Данные функции возвращают новый контекст, и  cancel функци. Cancel функцию можно использовать для отмены контекста. 

+ `ctx.WithTimeout()` - принимает на вход `time.Duration()`. Под капотом использует `ctx.WithDeadline()` с аргументом `time.Now+time.Duration`
+ `ctx.WithDeadline()` - принимает на вход `time.Time()`

## Sync pool

В высоконагруженных приложениях частое создание и удаление объектов приводит к:
1. Повышенной нагрузке на сборщик мусора.
2. Увеличению времени работы программы из-за затрат на выделение и освобождение памяти.

`sync.Pool` помогает решить эти проблемы, предоставляя возможность переиспользовать ранее созданные объекты вместо их удаления и повторного создания.

```
bufferPool := sync.Pool{  
    New: func() interface{} { return new(bytes.Buffer) },  
}  
some := bufferPool.Get()
```

+ Объекты в пуле могут быть удалены сборщиком мусора, если они не используются, поэтому `sync.Pool` не гарантирует сохранение всех объектов.
+ Обращение к пулу потокобезопасно
+ `sync.Pool` нельзя копировать после создания
+ Объекты помещаются в пул с помощью метода `Put`.
+ Извлекаются из пула с помощью метода `Get`.
+ Если `Get` вызывается, а пул пуст, создается новый объект с помощью функции `New`, заданной при создании пула.
+ `sync.Pool` не очищает объекты, при помещении из в pool. Поэтому до использования объектов, и необходимо обнулить. 

## Channels

Каналы - это примитив синхронизации, который позволяет безопасно обмениваться данными между горутинами. 

Создание канала:
```
package main
import "fmt"
func main() {
	var c chan int    
	fmt.Println(c)
} 
```


Чтение из канала 1:
```
for i := range c {  
    fmt.Println(i)  
}
```
+ при таком чтении из канала, цикл вычитает все значения и повиснет, если канал не закрыть. Если канал закрыть, то цикл завершится. 


Чтение из канала 2:
```
select {  
case <-c1:  
    // todo 
case <-c2:  
    // todo 
default:
	// default
}
```
+ данную конструкцию можно использовать для чтения из одного канала, либо другого. Такую конструкцию часто используют, для чтения канала контекста и какого либо другого. В этой конструкции нет порядка, то есть первым может считаться любое доступное значение из канала. 


При закрытии канала `close(c)`, из него можно считать бесконечное количество пустых значений. Если в закрытом канале остались какие либо значения, их можно считать, даже после того, как канал был закрыт. Можно проверить, валидно ли значение из канала с помощью конструкции:
```
x, ok := <-c1
```


Каналы бывают buffered и unbuffered:
+ buffered - канал, блокируется в том случае, если буфер заполнился
+ unbuffered-  канал будет заблокирован до тех пор, пока другая горутина не попытается считать данные из канала. 



## Обработка паник

## GC

## Interfaces

## Profiling

## Задачи
